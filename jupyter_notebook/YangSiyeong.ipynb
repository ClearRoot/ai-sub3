{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Project Sub3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-27c419a1547b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDEFINES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'configs'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from configs import DEFINES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋에 데이터 추가하기\n",
    "## http://www.aihub.or.kr/ 에서 데이터를 가져왔다.\n",
    "    - 엑셀 파일의 이름은 변경해야 한다.\n",
    "    - 엑셀의 내용 중 필요한 내용만 따로 빼서 csv파일로 만들었다.\n",
    "    - 길이가 너무 긴 질문이나 답은 제외 시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 열기\n",
    "names = ['academy.xlsx', 'accommodation.xlsx', 'caffe.xlsx', 'cloth.xlsx', 'estate.xlsx', 'food.xlsx', 'play.xlsx', 'service.xlsx', 'store.xlsx']\n",
    "\n",
    "for name in names:\n",
    "    wb = openpyxl.load_workbook(name)\n",
    "    # sheet 열기\n",
    "    sheet = wb['Sheet1']\n",
    "    # 마지막 row 셀 찾기\n",
    "    max_row = sheet.max_row\n",
    "    f = open('ChatBotData.csv', 'a', encoding='utf-8', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    for r in range(2, max_row):\n",
    "        # 질문과 답이 모두 있는것만 csv파일에 넣기\n",
    "        if sheet.cell(row=r, column=13).value and sheet.cell(row=r+1, column=16).value:\n",
    "            q = sheet.cell(row=r, column=13).value\n",
    "            a = sheet.cell(row=r+1, column=16).value\n",
    "            if len(q) < 25 and len(a) < 25:\n",
    "                wr.writerow([q, a, 0])\n",
    "    f.close()\n",
    "    wb.close()\n",
    "\n",
    "\n",
    "names = ['vehicle.xlsx', 'traffic.xlsx', 'waterworks.xlsx', 'passport.xlsx']\n",
    "for name in names:\n",
    "    wb = openpyxl.load_workbook(name)\n",
    "    sheet = wb['Sheet']\n",
    "    max_row = sheet.max_row\n",
    "    f = open('ChatBotData.csv', 'a', encoding='utf-8', newline='')\n",
    "    wr = csv.writer(f)\n",
    "    for r in range(2, max_row):\n",
    "        q = sheet.cell(row=r, column=5).value\n",
    "        a = sheet.cell(row=r, column=6).value\n",
    "        if q and a:\n",
    "            if len(q) < 25 and len(a) < 25:\n",
    "                wr.writerow([q, a, 0])\n",
    "    f.close()\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분석\n",
    "## wordcloud 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./ChatBotData.csv', encoding='utf-8')\n",
    "\n",
    "q_sentences = list(data['Q'])\n",
    "a_sentences = list(data['A'])\n",
    "\n",
    "# 토크나이즈\n",
    "q_token_sentences = []\n",
    "a_token_sentences = []\n",
    "\n",
    "okt = Okt() # 객체 생성\n",
    "\n",
    "for s in tqdm(q_sentences):\n",
    "    for token, tag in okt.pos(s.replace(' ', '')):\n",
    "        if tag == 'Noun':\n",
    "            q_token_sentences.append(token)\n",
    "            \n",
    "for s in tqdm(a_sentences):\n",
    "    for token, tag in okt.pos(s.replace(' ', '')):\n",
    "        if tag == 'Noun':\n",
    "            a_token_sentences.append(token)\n",
    "\n",
    "# 하나의 문자열로 만들기\n",
    "q_token_sentences = ' '.join(q_token_sentences)\n",
    "a_token_sentences = ' '.join(a_token_sentences)\n",
    "\n",
    "# # jupyter notebook 한글 폰트 \n",
    "# from matplotlib import font_manager, rc\n",
    "# font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# rc('font', family=font_name)\n",
    "\n",
    "# 질문관련 wordcloud\n",
    "q_wordcloud = WordCloud('C:/Windows/Fonts/NanumGothic.ttf').generate(q_token_sentences)\n",
    "\n",
    "plt.imshow(q_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 대답관련 wordcloud\n",
    "a_wordcloud = WordCloud('C:/Windows/Fonts/NanumGothic.ttf').generate(a_token_sentences)\n",
    "\n",
    "plt.imshow(a_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-1. 데이터를 읽고 트레이닝 셋과 테스트 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-1-1. 데이터를 읽고 트레이닝 셋과 테스트 셋으로 분리\n",
    "def load_data(file):\n",
    "    datas = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    q, a = list(datas['Q']), list(datas['A'])\n",
    "    \n",
    "    train_q, test_q, train_a, test_a = train_test_split(q, a, test_size=0.35, random_state=321)\n",
    "    return train_q, train_a, test_q, test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_q, train_a, test_q, test_a = load_data(\"./data_in/ChatBotData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-2. 텍스트 데이터에 정규화를 사용하여 ([~.,!?\\\"':;)(]) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_noise_canceling(sentence):\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    sentence = re.sub(RE_FILTER, \"\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-3. 텍스트 데이터에 토크나이징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 순서\n",
    "    - 텍스트 데이터 prepro_noise_canceling() 함수 처리\n",
    "    - 띄어쓰기 단위로 나누기\n",
    "    - 띄어진 단어들로 벡터 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_data(data):\n",
    "    data = prepro_noise_canceling(data)\n",
    "    data_splited = list(data.split())\n",
    "    \n",
    "    return data_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 내용들이 적용되어야 함\n",
    "    - 텍스트 데이터 prepro_noise_canceling()함수 처리\n",
    "    - 문장을 토큰 단위로 나누기\n",
    "    - dictionary를 활용하여 토큰 인덱스화\n",
    "    - dictionary에 없는 토큰의 경우 UNK 값으로 대체\n",
    "    - 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기\n",
    "    - 기준 문장 길이에 맞게 남은 공간이 있다면 pedding 하기\n",
    "    - 문장 인덱스와 문장 길이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리\n",
    "def enc_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "    \n",
    "    for seq in value:\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        for word in seq.split():\n",
    "            if dictionary.get(word) is not None:\n",
    "                # seq_index에 dictionary 안의 인덱스를 extend 한다\n",
    "                seq_index.append(dictionary[word])\n",
    "            else:\n",
    "                # dictionary에 존재 하지 않는 다면 UNK 값을 extend 한다\n",
    "                seq_index.append(dictionary[UNK])\n",
    "\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        if len(seq_index) > DEFINES.max_sequence_length:\n",
    "            seq_index = seq_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += (DEFINES.max_sequence_length - len(seq_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "\n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-2. 디코더에 필요한 데이터 전 처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 내용들이 적용되어야 함\n",
    "    - 텍스트 데이터 prepro_noise_canceling()함수 처리\n",
    "    - 문장을 토큰 단위로 나누기\n",
    "    - dictionary를 활용하여 토큰 인덱스화\n",
    "    - 첫 인덱스에 STD 추가\n",
    "    - dictionary에 없는 토큰의 경우 UNK 값으로 대체\n",
    "    - 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기\n",
    "    - 기준 문장 길이에 맞게 남은 공간이 있다면 pedding 하기\n",
    "    - 문장 인덱스와 문장 길이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dec_input_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "\n",
    "    for seq in value:\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        for word in seq.split():\n",
    "            # 디코딩 입력의 처음에는 START가 와야 하므로 STD 값 추가\n",
    "            seq_index.append(dictionary[STD])\n",
    "\n",
    "            if dictionary.get(word) is not None:\n",
    "                # seq_index에 dictionary 안의 인덱스를 extend 한다\n",
    "                seq_index.append(dictionary[word])\n",
    "            else:\n",
    "                # dictionary에 존재 하지 않는 다면 seq_index에 UNK 값을 extend 한다\n",
    "                seq_index.append(dictionary[UNK])\n",
    "\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        if len(seq_index) > DEFINES.max_sequence_length:\n",
    "            seq_index = seq_index[:DEFINES.max_sequence_length]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += (DEFINES.max_sequence_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "    \n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-3. 디코더에 필요한 데이터 전 처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-2-3. 디코더에 필요한 데이터 전 처리 \n",
    "def dec_target_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    value = prepro_noise_canceling(value)\n",
    "\n",
    "    for seq in value:\n",
    "\n",
    "        # 하나의 seq에 index를 저장할 배열 초기화\n",
    "        seq_index =[]\n",
    "\n",
    "        seq_index = [dictionary[word] for word in seq.split()]\n",
    "        # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "        # END 토큰을 추가 (DEFINES.max_sequence_length 길이를 맞춰서 추가)\n",
    "        \n",
    "        seq_index = seq_index[:DEFINES.max_sequence_length-1] + [dictionary[END]]\n",
    "\n",
    "        # seq의 길이를 저장\n",
    "        seq_len.append(len(seq_index))\n",
    "\n",
    "        # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "        seq_index += ( DEFINES.max_sequence_length - len(seq_index)) * [dictionary[PAD]]\n",
    "\n",
    "        # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "        seq_input_index.append(seq_index)\n",
    "   \n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
