{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Project Sub3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from configs import DEFINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<PAD>\"\n",
    "STD = \"<SOS>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-1. 데이터를 읽고 트레이닝 셋과 테스트 셋으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    datas = pd.read_csv(file, skiprows=[0,0], header=None)\n",
    "    q = datas[0]\n",
    "    a = datas[1]\n",
    "    train_q, test_q, train_a, test_a = train_test_split(q, a, test_size=0.35, random_state=321)\n",
    "    \n",
    "    return train_q, train_a, test_q, test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_q, train_a, test_q, test_a = load_data(\"./data_in/ChatBotData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-2. 텍스트 데이터에 정규화를 사용하여 ([~.,!?\\\"':;)(]) 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_noise_canceling(sentence):\n",
    "    RE_FILTER = re.compile(\"[.,!?\\\"':;~()]\")\n",
    "    sentence = re.sub(RE_FILTER, \"\", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-1-3. 텍스트 데이터에 토크나이징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 순서\n",
    "    - 텍스트 데이터 prepro_noise_canceling() 함수 처리\n",
    "    - 띄어쓰기 단위로 나누기\n",
    "    - 띄어진 단어들로 벡터 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_data(data):\n",
    "    data = prepro_noise_canceling(data)\n",
    "    data_splited = list(data.split())\n",
    "    \n",
    "    return data_splited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 내용들이 적용되어야 함\n",
    "    - 텍스트 데이터 prepro_noise_canceling()함수 처리\n",
    "    - 문장을 토큰 단위로 나누기\n",
    "    - dictionary를 활용하여 토큰 인덱스화\n",
    "    - dictionary에 없는 토큰의 경우 UNK 값으로 대체\n",
    "    - 기준 문장 길이 보다 크게 된다면 뒤의 토큰 자르기\n",
    "    - 기준 문장 길이에 맞게 남은 공간이 있다면 pedding 하기\n",
    "    - 문장 인덱스와 문장 길이 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Req 1-2-1. 토큰화된 트레이닝 데이터를 인코더에 활용할 수 있도록 전 처리\n",
    "def enc_processing(value, dictionary):\n",
    "    \n",
    "    # 인덱스 정보를 저장할 배열 초기화\n",
    "    seq_input_index = []\n",
    "    # 문장의 길이를 저장할 배열 초기화\n",
    "    seq_len = []\n",
    "    # 노이즈 캔슬\n",
    "    for val in value:\n",
    "        val = prepro_noise_canceling(val)\n",
    "    \n",
    "        for seq in val:\n",
    "            # 하나의 seq에 index를 저장할 배열 초기화\n",
    "            seq_index =[]\n",
    "\n",
    "            for word in seq.split():\n",
    "                if dictionary.get(word) is not None:\n",
    "                    # seq_index에 dictionary 안의 인덱스를 extend 한다\n",
    "                    seq_index.extend(dictionary[word])\n",
    "                else:\n",
    "                    # dictionary에 존재 하지 않는 다면 UNK 값을 extend 한다\n",
    "                    seq_index.extend(dictionary[UNK])\n",
    "\n",
    "            # 문장 제한 길이보다 길어질 경우 뒤에 토큰을 제거\n",
    "            if len(seq_index) > DEFINES.max_sequence_length:\n",
    "                seq_index = seq_index[:,DEFINES.max_sequence_length]\n",
    "       \n",
    "            # seq의 길이를 저장\n",
    "            seq_len.append(len(seq_index))\n",
    "            \n",
    "            # DEFINES.max_sequence_length 길이보다 작은 경우 PAD 값을 추가 (padding)\n",
    "            while DEFINES.max_sequence_length < seq_len:\n",
    "                seq_index += (DEFINES.max_sequence_length - seq_len) * PAD_INDEX\n",
    "\n",
    "            # 인덱스화 되어 있는 값은 seq_input_index에 추가\n",
    "            seq_input_index.append(seq_index)\n",
    "        \n",
    "    return np.asarray(seq_input_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
